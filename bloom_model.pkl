import pandas as pd

# Load the dataset
file_path = 'synthetic_algae_data.csv'
algae_data = pd.read_csv(file_path)

# Inspect the dataset
print(algae_data.head())
print(algae_data.info())
print(algae_data.describe())

# Step 1: Handle Missing Values
# Dropping rows with missing values (adjust as per your dataset's needs)
algae_data = algae_data.dropna()

# Step 2: One-Hot Encode Categorical Columns
# Identifying the categorical column
categorical_columns = ['Algae Species']

# One-hot encode the categorical columns
algae_data = pd.get_dummies(algae_data, columns=categorical_columns)

# Step 3: Feature Scaling (Standardizing Numerical Data)
from sklearn.preprocessing import StandardScaler

# Identify feature columns (exclude the target column, e.g., 'Growth Rate (g/m²/day)')
target_column = 'Growth Rate (g/m²/day)'
feature_columns = [col for col in algae_data.columns if col != target_column]

# Standardize features (only the numerical features)
scaler = StandardScaler()
algae_data[feature_columns] = scaler.fit_transform(algae_data[feature_columns])

print("Preprocessing completed.")
from sklearn.model_selection import train_test_split

# Define the target column and feature columns
target_column = 'Growth Rate (g/m²/day)'
X = algae_data.drop(columns=[target_column])  # Features
y = algae_data[target_column]  # Target

# Split the dataset into training and testing sets (80% training, 20% testing)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print("Data splitting completed.")
from sklearn.ensemble import RandomForestRegressor

model = RandomForestRegressor(n_estimators=100, random_state=42)
model.fit(X_train, y_train)
from sklearn.metrics import mean_squared_error, r2_score

y_pred = model.predict(X_test)
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f'MSE: {mse}')
print(f'R² Score: {r2}')
from sklearn.model_selection import GridSearchCV

param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5, 10]
}

grid_search = GridSearchCV(RandomForestRegressor(random_state=42), param_grid, cv=5, scoring='r2')
grid_search.fit(X_train, y_train)

print(grid_search.best_params_)
import numpy as np
import pandas as pd

# Example: New data for prediction (replace with your actual values)
new_data = pd.DataFrame({
    'CO2 Levels (ppm)': [450],
    'Temperature (°C)': [22],
    'Humidity (%)': [75],
    'Light Intensity (lux)': [300],
    'Nutrient Levels (mg/L)': [5],
    'Water pH': [7.5],
    'Algae Species': ['Species A']  # Replace with the actual species name
})

# Perform one-hot encoding on the new data for 'Algae Species'
new_data_encoded = pd.get_dummies(new_data, columns=['Algae Species'])

# Ensure the new data has the same columns as the training data
# Get the columns that were used during training, and match the new data
missing_cols = set(X.columns) - set(new_data_encoded.columns)
for col in missing_cols:
    new_data_encoded[col] = 0  # Add missing columns with 0s (for species not present)

# Reorder the columns to match the order of the training data
new_data_encoded = new_data_encoded[X.columns]

# Scale the data (use the same scaler as during training)
new_data_scaled = scaler.transform(new_data_encoded)

# Load the trained model
loaded_model = joblib.load('algae_growth_predictor.pkl')

# Predict the algae growth rate using the trained model
prediction = loaded_model.predict(new_data_scaled)
print(f'Predicted Growth Rate: {prediction[0]}')
